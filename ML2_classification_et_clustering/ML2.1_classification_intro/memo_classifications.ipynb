{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97982cf6",
   "metadata": {},
   "source": [
    "# <font color= 'darkorchid'> Mémos Classification </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c5582e",
   "metadata": {},
   "source": [
    "## <font color= 'mediumorchid'> Preprocessing </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf9c8d",
   "metadata": {},
   "source": [
    "* **classes** : \n",
    "    * encode the target classe 0 or 1. <font color='red'> **1 = positif** </font>                            \n",
    "    * look the balance between classes : same number of samples ? *ex: same number of patients that are sick and healthy*\n",
    "    if imbalance : use other model as SVM \n",
    "\n",
    "* **categorical features** : convert into binary value 0-1 or ordinal value : ordinal value if there is a distance between these numbers !\n",
    "\n",
    "* **continuous features** : Scaler() : center and standardize the values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e68f0b4",
   "metadata": {},
   "source": [
    "## <font color= 'mediumorchid'> Classification with the K-nearest neighbors (KNN) </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece1a5f4",
   "metadata": {},
   "source": [
    "1) At training, it simply memorizes all the training samples features $X$ and classes $y$. \n",
    "\n",
    "2) At test time given the features of one sample $x'$, it identifies the $k$ training samples $x_i, i \\in 1,\\dots,k$ that are the closest to $x'$ (in euclidian distance), and assign the class $y'$ that is the most frequent among the k-neareast neighbor classes $y_i, i \\in 1,\\dots,k$.\n",
    "\n",
    "So <font color='red'> **each test sample is assigned a probablity and knn simply predicts 1 if this probability is higher than > 0.5** </font>\n",
    "\n",
    "*ex: the probability of having a heart disease is the proportion of the k-nearest train samples that have a heart disease.*\n",
    "\n",
    "### <font color= 'plum'> **Confusion matrix** </font>\n",
    "* *true positives* (TP) : the number of patients that have been correctly classified as having a disease : \n",
    "* *true negatives* (TN) : the number of patients that have been correctly classified as not having a disease \n",
    "*  *false positives* (FP) : the number of patients that have been incorrectly classified as having a disease \n",
    "* *false negatives* (FN) : the number of patients that have been incorrectly classified as nothaving a disease \n",
    "\n",
    "/!\\ the <font color= 'blue' > true/false </font> refers to <font color= 'blue' > the *true* class of the test samples </font>, whereas  <font color= 'green'> the positive/negative </font> refers to <font color= 'green'> the *predicted* class </font>  by the classifier.\n",
    "\n",
    "The confusion matrix gives these four numbers in the following format:\n",
    "\n",
    "|  |  |\n",
    "|--|--|\n",
    "|TN|FP|\n",
    "|FN|TP|\n",
    "\n",
    "The best : maximize the TP and TN ! The objectif is to decrease the FP and FN.              \n",
    "\n",
    "`confusion_matrix(y_test, y_test_pred)`             \n",
    "*toujours arg 1 : lignes et arg2 : colonnes dans la confusion matrix* \n",
    "\n",
    "### <font color= 'plum'> **predict proba** </font>\n",
    "\n",
    "`y_test_proba = knn_clf.predict_proba(X_test_scaled)`                                   \n",
    "Results :                                                       \n",
    "```python\n",
    "array([[0.93333333, 0.06666667],                                \n",
    "        [0.        , 1.        ],                                    \n",
    "        [0.86666667, 0.13333333],                    \n",
    "        [0.33333333, 0.66666667],                                    \n",
    "         [0.2       , 0.8       ],                            \n",
    "        [0.26666667, 0.73333333],                                \n",
    "        [0.66666667, 0.33333333]])  \n",
    "```\n",
    "Explanation : \n",
    "* left : probability to be in classe 0 --> negatif *not sick* \n",
    "* right : probability to be in classe 1 --> positif *sick*  \n",
    "\n",
    "\n",
    "### <font color= 'plum'> **threshold** </font>\n",
    "\n",
    "As mentionned each test sample is assigned a probablity and knn simply predicts 1 if this probability is higher than > 0.5. \n",
    "* if we increase the threshold : we increase the precision\n",
    "* if we drecrease the threshold : we increase the recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1a445",
   "metadata": {},
   "source": [
    "## <font color= 'mediumorchid'> Scoring of classification </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e127083",
   "metadata": {},
   "source": [
    "### <font color='plum'> **Accuracy_score** </font>\n",
    "         \n",
    "`accuracy_score(y_test, y_test_pred)`\n",
    "\n",
    "* Is calculated on the <font color='coral'> **predicted classes** </font>\n",
    "* It measures how many observations, both positive and negative, were correctly classified.               \n",
    "* Need to apply a certain threshold before computing it.   \n",
    "\n",
    "So, when does it make sense to use it? \n",
    "* When your problem is balanced, using accuracy is usually a good start. \n",
    "* An additional benefit is that it is really easy to explain it to non-technical stakeholders in your project.\n",
    "* When every class is equally important to you.\n",
    "\n",
    "*ex : accuracy = 0.81 = 81% of the diagonal TP and TN are  classified. The rest is not but we don't know where* \n",
    "\n",
    "### <font color='plum'> **Recall** </font>\n",
    "Important when we want to **minimize the FN** i.e. *people that are sick but predicted not*\n",
    "To the detriment of FP (*people not sick but predicted as sick*) : decrease **precision**.          \n",
    "This means we'd prefer **to have a higher Recall, to the cost of having a lower Precision**.        \n",
    "And that implies **choosing a threshold that is lower than 0.5 for assigning the classes.**             \n",
    "*i.e. : if threshold lower that 0.5, more test samples are going to be classifed as 1 = sick people*\n",
    "\n",
    "`recall_score (y_test, y_test_pred)`\n",
    "\n",
    "* Is calculated on the <font color='coral'> **predicted classes** </font>\n",
    "* **i don't want to miss +** even if i am not ! Increase FP. I want to catch all the + even if i detect - as +. \n",
    "\n",
    "### <font color='plum'> **Precision** </font>\n",
    "\n",
    "`precision_score (y_test, y_test_pred)`\n",
    "\n",
    "* Is calculated on the <font color='coral'> **predicted classes** </font>\n",
    "* what is trully +  for all the samples classified. What is the truth part of + and how much can I trust my model to predict + ? \n",
    "* **I want to really be sure to be +** and not say i am + if it is not (and detect + if it is trully + --> increase FN) \n",
    "* *ex: pregnancy test : be sure at 99.9% to be + !!* \n",
    "* **trade_off recall/precision** : both evolve on the contrary \n",
    "\n",
    "### <font color='plum'> **F1_score** </font>\n",
    "\n",
    "`f1_score(y_test, y_test_pred)`\n",
    "\n",
    "* Is calculated on the <font color='coral'> **predicted classes** </font>\n",
    "* compromise between precision and recall \n",
    "* calculating the harmonic mean between those two. \n",
    "* beta : \n",
    "    * with the F1 score, we care equally about recall and precision; \n",
    "    * with the F2 score, recall is twice as important to us.\n",
    "    * with 0<beta<1, we care more about precision, and so the higher the threshold, the higher the F beta score. \n",
    "    * when beta > 1, our optimal threshold moves toward lower thresholds\n",
    "    * when beta = 1, it is somewhere in the middle.\n",
    "\n",
    "\n",
    "### <font color='plum'> **average precision score / PR AUC** </font>\n",
    "`average_precision_score(y_test, y_score)`\n",
    "\n",
    "* Is calculated on the <font color='mediumturquoise'> **prediction score/proba** </font>\n",
    "* *y_score* : Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions\n",
    "* take in account the trade-off between precision and recall\n",
    "* precision-recall curve : It is a curve that combines precision (PPV) and recall (TPR) in a single visualization. For every threshold, you calculate PPV and TPR and plot them. The higher the y-axis on your curve, the better your model’s performance.\n",
    "* you can calculate the area under the precision-recall curve (PR AUC) to get one number that describes model performance : AP \n",
    "\n",
    "\n",
    "### <font color='plum'> **ROC AUC score** </font>\n",
    "`roc_auc_score(y_test, y_score)`\n",
    "\n",
    "* Is calculated on the <font color='mediumturquoise'> **prediction score/proba** </font>\n",
    "* roc : It is a chart that visualizes the trade-off between the true positive rate (TPR) and the false positive rate (FPR). Basically, for every threshold, we calculate TPR and FPR and plot them on one chart.\n",
    "* auc : calcul the area under the curve: **maximize the auc = minimize the FP** \n",
    "* average scoring that takes in account the FP ! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951812a4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
